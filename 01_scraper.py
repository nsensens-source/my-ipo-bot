import os
import pandas as pd
import requests
from supabase import create_client
from io import StringIO

# --- ‚öôÔ∏è CONFIG & ENVIRONMENT ---
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

if GITHUB_TOKEN:
    HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"
    
IS_TEST_MODE = os.getenv("TEST_MODE", "Off").strip().lower() == "on"
TABLE_NAME = "ipo_trades_uat" if IS_TEST_MODE else "ipo_trades"

if IS_TEST_MODE:
    print(f"\nüß™ TEST MODE: ON -> Using table '{TABLE_NAME}'")
else:
    print(f"\nüü¢ PROD MODE -> Using table '{TABLE_NAME}'")

REPO_BASE_URL = "https://raw.githubusercontent.com/nsensens-source/my-ipo-bot/main"

# ---------------------------------------------------------
# 1. ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)
# ---------------------------------------------------------
def get_external_sp500():
    print("üá∫üá∏ Fetching S&P 500 (Base)...")
    try:
        url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
        df = pd.read_csv(url)
        return [{"ticker": s.replace('.', '-').strip(), "market_type": "SP500_BASE"} for s in df['Symbol']]
    except: return []

def get_external_thai_set100():
    print("üáπüá≠ Fetching SET100 (Base)...")
    try:
        url = "https://en.wikipedia.org/wiki/SET100_Index"
        response = requests.get(url, headers=HEADERS)
        dfs = pd.read_html(StringIO(response.text))
        tickers = []
        for df in dfs:
            if 'Symbol' in df.columns:
                for s in df['Symbol']:
                    clean_s = str(s).strip()
                    if not clean_s.endswith(".BK"): clean_s += ".BK"
                    tickers.append({"ticker": clean_s, "market_type": "SET_BASE"})
                break
        return tickers
    except: return []

# ---------------------------------------------------------
# 2. ‡∏ô‡∏±‡∏Å‡∏•‡πà‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ã‡∏¥‡πà‡∏á (‡∏™‡∏π‡∏ï‡∏£‡πÉ‡∏´‡∏°‡πà: 50 US / 40 TH) ‚öñÔ∏è
# ---------------------------------------------------------
def get_market_movers():
    print("üöÄ Scanning Market Movers (Balanced Strategy)...")
    tickers = []
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á (Limit)
    targets = [
        # --- üá∫üá∏ US MARKET (Total 50) ---
        # 1. US Gainers (25 ‡∏ï‡∏±‡∏ß) -> ‡∏Ç‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô
        ("https://finance.yahoo.com/gainers", "AUTO_LONG_US", 25),
        # 2. US Losers (25 ‡∏ï‡∏±‡∏ß) -> ‡∏Ç‡∏≤‡∏•‡∏á (Short/Rebound)
        ("https://finance.yahoo.com/losers", "AUTO_SHORT_US", 25),
        
        # --- üáπüá≠ THAI MARKET (Total 40) ---
        # 3. TH Gainers (20 ‡∏ï‡∏±‡∏ß) -> ‡∏Ç‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô
        ("https://finance.yahoo.com/gainers?region=TH", "AUTO_LONG_TH", 20),
        # 4. TH Losers (20 ‡∏ï‡∏±‡∏ß) -> ‡∏Ç‡∏≤‡∏•‡∏á
        ("https://finance.yahoo.com/losers?region=TH", "AUTO_SHORT_TH", 20)
    ]
    
    for url, m_type, limit in targets:
        print(f"   üëâ Scraping {m_type} (Limit: {limit})...")
        try:
            response = requests.get(url, headers=HEADERS)
            dfs = pd.read_html(StringIO(response.text))
            if not dfs: continue 

            df = dfs[0]
            
            # ‡∏´‡∏≤ Column ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô
            symbol_col = None
            possible_names = ['Symbol', 'Ticker', '‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠', '‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå']
            for col in df.columns:
                if col in possible_names:
                    symbol_col = col
                    break
            if not symbol_col: symbol_col = df.columns[0]
            
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Limit ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ (25 ‡∏´‡∏£‡∏∑‡∏≠ 20)
            count_found = 0
            for raw_symbol in df[symbol_col]:
                if count_found >= limit: break # ‡∏Ñ‡∏£‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏¢‡∏∏‡∏î
                
                symbol_str = str(raw_symbol).strip()
                
                # --- LOGIC ‡πÅ‡∏¢‡∏Å‡∏™‡∏±‡∏ç‡∏ä‡∏≤‡∏ï‡∏¥ ---
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î‡πÑ‡∏ó‡∏¢ (URL ‡∏°‡∏µ region=TH)
                if "_TH" in m_type:
                    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ .BK (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ)
                    if ".BK" not in symbol_str:
                        final_ticker = f"{symbol_str}.BK"
                    else:
                        final_ticker = symbol_str
                    
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ß (.F) ‡∏´‡∏£‡∏∑‡∏≠ Warrant (.W) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏•‡πà‡∏ô
                    if ".F.BK" in final_ticker: continue 
                    
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î US
                else:
                    final_ticker = symbol_str
                    # ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏µ .BK ‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î US (‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏¢‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏Å‡∏±‡∏ô‡πÑ‡∏ß‡πâ) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
                    if ".BK" in final_ticker: continue

                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
                if "^" in final_ticker or "USD" in final_ticker: continue

                tickers.append({"ticker": final_ticker, "market_type": m_type})
                count_found += 1
                
        except Exception as e:
            print(f"      ‚ö†Ô∏è Error: {e}")
            pass
        
    return tickers

# ---------------------------------------------------------
# 3. User Manual
# ---------------------------------------------------------
def get_user_manual_list(filename, type_name):
    print(f"üåï Fetching '{filename}' from User GitHub...")
    tickers = []
    try:
        url = f"{REPO_BASE_URL}/{filename}"
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            lines = response.text.splitlines()
            clean_lines = [line.strip() for line in lines if line.strip() and not line.startswith("#")]
            tickers = [{"ticker": t, "market_type": type_name} for t in clean_lines]
            print(f"   ‚úÖ Found {len(tickers)} items in {filename}")
    except: pass
    return tickers

# ---------------------------------------------------------
# MAIN
# ---------------------------------------------------------
def main():
    print("ü§ñ Starting Balanced Scraper...")
    
    # 1. Base (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏î‡∏π‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°)
    base_data = get_external_sp500() + get_external_thai_set100()
    
    # 2. Hunters (‡∏û‡∏£‡∏∞‡πÄ‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤: 90 ‡∏ï‡∏±‡∏ß)
    hunter_data = get_market_movers()
    
    # 3. Manual
    manual_data = get_user_manual_list("moonshots.txt", "MOONSHOT") + \
                  get_user_manual_list("favourites.txt", "FAVOURITE")
    
    all_data = base_data + hunter_data + manual_data
    
    if not all_data:
        print("‚ö†Ô∏è No data found!")
        return

    print(f"\nüíæ Syncing {len(all_data)} tickers to Supabase...")
    
    count = 0
    for item in all_data:
        try:
            supabase.table(TABLE_NAME).upsert({
                "ticker": item['ticker'],
                "market_type": item['market_type'],
                "status": "watching"
            }, on_conflict="ticker").execute()
            count += 1
            if count % 100 == 0: print(f"   ...synced {count}")
        except: pass

    print(f"‚úÖ SUCCESS: Synced {count} tickers.")
    print(f"   - Base Markets: {len(base_data)}")
    print(f"   - Hunters (Active): {len(hunter_data)}")
    print(f"   - User Manual: {len(manual_data)}")

if __name__ == "__main__":
    main()
