import os
import pandas as pd
import requests
import yfinance as yf  # à¹€à¸à¸´à¹ˆà¸¡ yfinance à¸ªà¸³à¸«à¸£à¸±à¸šà¸”à¸¶à¸‡à¸£à¸²à¸„à¸²à¸«à¸¸à¹‰à¸™à¹„à¸—à¸¢
from supabase import create_client
from io import StringIO

# --- âš™ï¸ CONFIG & ENVIRONMENT ---
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

if GITHUB_TOKEN:
    HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"
    
IS_TEST_MODE = os.getenv("TEST_MODE", "Off").strip().lower() == "on"
TABLE_NAME = "ipo_trades_uat" if IS_TEST_MODE else "ipo_trades"

if IS_TEST_MODE:
    print(f"\nğŸ§ª TEST MODE: ON -> Using table '{TABLE_NAME}'")
else:
    print(f"\nğŸŸ¢ PROD MODE -> Using table '{TABLE_NAME}'")

REPO_BASE_URL = "https://raw.githubusercontent.com/nsensens-source/my-ipo-bot/main"

# ---------------------------------------------------------
# 1. à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸¥à¸²à¸”à¸«à¸¥à¸±à¸
# ---------------------------------------------------------
def get_external_sp500():
    print("ğŸ‡ºğŸ‡¸ Fetching S&P 500 (Base)...")
    try:
        url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
        df = pd.read_csv(url)
        return [{"ticker": s.replace('.', '-').strip(), "market_type": "SP500_BASE"} for s in df['Symbol']]
    except: return []

def get_external_thai_set100():
    print("ğŸ‡¹ğŸ‡­ Fetching SET100 (Base)...")
    try:
        url = "https://en.wikipedia.org/wiki/SET100_Index"
        response = requests.get(url, headers=HEADERS)
        dfs = pd.read_html(StringIO(response.text))
        tickers = []
        for df in dfs:
            if 'Symbol' in df.columns:
                for s in df['Symbol']:
                    clean_s = str(s).strip()
                    if not clean_s.endswith(".BK"): clean_s += ".BK"
                    tickers.append({"ticker": clean_s, "market_type": "SET_BASE"})
                break
        return tickers
    except: return []

# ---------------------------------------------------------
# 2. à¸™à¸±à¸à¸¥à¹ˆà¸²à¸«à¸¸à¹‰à¸™à¸‹à¸´à¹ˆà¸‡ US (à¸à¸§à¸²à¸”à¸ˆà¸²à¸à¹€à¸§à¹‡à¸š)
# ---------------------------------------------------------
def get_us_market_movers():
    print("ğŸš€ Scanning US Market Movers...")
    tickers = []
    targets = [
        ("https://finance.yahoo.com/gainers", "AUTO_LONG_US", 25),
        ("https://finance.yahoo.com/losers", "AUTO_SHORT_US", 25)
    ]
    for url, m_type, limit in targets:
        print(f"   ğŸ‘‰ Scraping {m_type}...")
        try:
            response = requests.get(url, headers=HEADERS)
            dfs = pd.read_html(StringIO(response.text))
            if not dfs: continue 

            df = dfs[0]
            symbol_col = next((col for col in df.columns if col in ['Symbol', 'Ticker', 'à¸Šà¸·à¹ˆà¸­à¸¢à¹ˆà¸­']), df.columns[0])
            
            count = 0
            for raw_symbol in df[symbol_col]:
                if count >= limit: break
                final_ticker = str(raw_symbol).strip()
                if ".BK" in final_ticker or "^" in final_ticker or "USD" in final_ticker: continue
                tickers.append({"ticker": final_ticker, "market_type": m_type})
                count += 1
        except Exception as e:
            print(f"      âš ï¸ Error US: {e}")
    return tickers

# ---------------------------------------------------------
# 3. à¸™à¸±à¸à¸¥à¹ˆà¸²à¸«à¸¸à¹‰à¸™à¸‹à¸´à¹ˆà¸‡ TH (à¸„à¸³à¸™à¸§à¸“à¹€à¸­à¸‡à¸ˆà¸²à¸ SET100 à¹à¸šà¸šà¹à¸¡à¹ˆà¸™à¸¢à¸³) âš–ï¸
# ---------------------------------------------------------
def get_thai_market_movers(limit=20):
    print("ğŸ‡¹ğŸ‡­ Scanning Thai Market Movers (Self-Calculated from SET100)...")
    tickers_data = []
    try:
        # à¹ƒà¸Šà¹‰à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¸ˆà¸²à¸ SET100 à¸—à¸µà¹ˆà¹€à¸£à¸²à¸¡à¸µ
        set100_list = [item['ticker'] for item in get_external_thai_set100()]
        if not set100_list: return []

        print(f"   ğŸ‘‰ Downloading data for {len(set100_list)} Thai stocks...")
        # à¹‚à¸«à¸¥à¸”à¸£à¸²à¸„à¸²à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸£à¸§à¸”à¹€à¸”à¸µà¸¢à¸§à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹„à¸§
        data = yf.download(set100_list, period="5d", progress=False)
        
        if 'Close' in data:
            close_data = data['Close']
            for ticker in set100_list:
                if ticker in close_data.columns:
                    col = close_data[ticker].dropna()
                    if len(col) >= 2:
                        prev_price = float(col.iloc[-2])
                        curr_price = float(col.iloc[-1])
                        if prev_price > 0:
                            pct = ((curr_price - prev_price) / prev_price) * 100
                            tickers_data.append({"ticker": ticker, "pct_change": pct})
                            
        # à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¸«à¸²à¸•à¸±à¸§à¸—à¹‡à¸­à¸›
        tickers_data.sort(key=lambda x: x['pct_change'], reverse=True)
        results = []
        
        # 20 à¸­à¸±à¸™à¸”à¸±à¸šà¹à¸£à¸ (à¸šà¸§à¸à¹€à¸¢à¸­à¸°à¸ªà¸¸à¸”) -> AUTO_LONG_TH
        for item in tickers_data[:limit]:
            results.append({"ticker": item["ticker"], "market_type": "AUTO_LONG_TH"})
            
        # 20 à¸­à¸±à¸™à¸”à¸±à¸šà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ (à¸¥à¸šà¹€à¸¢à¸­à¸°à¸ªà¸¸à¸”) -> AUTO_SHORT_TH
        for item in tickers_data[-limit:]:
            results.append({"ticker": item["ticker"], "market_type": "AUTO_SHORT_TH"})
            
        print(f"   âœ… Found {limit} Thai Gainers and {limit} Thai Losers.")
        return results
    except Exception as e:
        print(f"      âš ï¸ Error Thai Movers: {e}")
        return []

# ---------------------------------------------------------
# 4. User Manual
# ---------------------------------------------------------
def get_user_manual_list(filename, type_name):
    print(f"ğŸŒ• Fetching '{filename}' from User GitHub...")
    tickers = []
    try:
        url = f"{REPO_BASE_URL}/{filename}"
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            lines = response.text.splitlines()
            clean_lines = [line.strip() for line in lines if line.strip() and not line.startswith("#")]
            tickers = [{"ticker": t, "market_type": type_name} for t in clean_lines]
            print(f"   âœ… Found {len(tickers)} items in {filename}")
    except: pass
    return tickers

# ---------------------------------------------------------
# MAIN
# ---------------------------------------------------------
def main():
    print("ğŸ¤– Starting Balanced Scraper...")
    
    # 1. Base Market
    base_data = get_external_sp500() + get_external_thai_set100()
    
    # 2. Hunters (US + TH)
    hunter_data = get_us_market_movers() + get_thai_market_movers(limit=20)
    
    # 3. Manual Lists
    manual_data = get_user_manual_list("moonshots.txt", "MOONSHOT") + \
                  get_user_manual_list("favourites.txt", "FAVOURITE")
    
    all_data = base_data + hunter_data + manual_data
    
    if not all_data:
        print("âš ï¸ No data found!")
        return

    print(f"\nğŸ’¾ Syncing {len(all_data)} tickers to Supabase...")
    
    count = 0
    for item in all_data:
        try:
            supabase.table(TABLE_NAME).upsert({
                "ticker": item['ticker'],
                "market_type": item['market_type'],
                "status": "watching"
            }, on_conflict="ticker").execute()
            count += 1
            if count % 100 == 0: print(f"   ...synced {count}")
        except: pass

    print(f"âœ… SUCCESS: Synced {count} tickers.")

if __name__ == "__main__":
    main()
