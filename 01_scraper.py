import os
import pandas as pd
import requests
from supabase import create_client

# --- CONFIGURATION ---
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

# Headers ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Yahoo/Wiki ‡∏ö‡∏•‡πá‡∏≠‡∏Å
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ‡πÉ‡∏™‡πà URL ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå GitHub ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Raw Link)
REPO_BASE_URL = "https://raw.githubusercontent.com/YOUR_GITHUB_USER/YOUR_REPO/main"

# ---------------------------------------------------------
# 1. ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å (External Sources Only)
# ---------------------------------------------------------

def get_external_sp500():
    """‡∏î‡∏∂‡∏á S&P 500 ‡∏à‡∏≤‡∏Å GitHub CSV (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Hardcode)"""
    print("üá∫üá∏ Fetching S&P 500 from External CSV...")
    try:
        url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
        df = pd.read_csv(url)
        return [{"ticker": s.replace('.', '-').strip(), "market_type": "SP500_BASE"} for s in df['Symbol']]
    except: return []

def get_external_nasdaq100():
    """‡∏î‡∏∂‡∏á NASDAQ-100 ‡∏à‡∏≤‡∏Å Wikipedia (Dynamic Parsing)"""
    print("üíª Fetching NASDAQ-100 from Wikipedia...")
    try:
        dfs = pd.read_html('https://en.wikipedia.org/wiki/Nasdaq-100')
        for df in dfs:
            if 'Ticker' in df.columns:
                return [{"ticker": s.strip(), "market_type": "NASDAQ_BASE"} for s in df['Ticker']]
    except: return []

# ---------------------------------------------------------
# 2. ‡∏ô‡∏±‡∏Å‡∏•‡πà‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ã‡∏¥‡πà‡∏á (Dynamic Hunters - Yahoo Finance)
# ---------------------------------------------------------

def get_market_movers():
    """
    ‡∏î‡∏∂‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ã‡∏¥‡πà‡∏á (Gainers/Losers/Active) ‡∏à‡∏≤‡∏Å Yahoo Finance
    ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Region (US ‡πÅ‡∏•‡∏∞ TH) ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    """
    print("üöÄ Scanning Market Movers (US & Thai)...")
    tickers = []
    
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ URL ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏≠‡∏á)
    targets = [
        # ‡∏ï‡∏•‡∏≤‡∏î US
        ("https://finance.yahoo.com/gainers", "AUTO_LONG_US"),
        ("https://finance.yahoo.com/losers", "AUTO_SHORT_US"),
        ("https://finance.yahoo.com/most-active", "AUTO_ACTIVE_US"),
        # ‡∏ï‡∏•‡∏≤‡∏î‡πÑ‡∏ó‡∏¢ (‡πÉ‡∏ä‡πâ region=TH ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        ("https://finance.yahoo.com/most-active?region=TH", "AUTO_ACTIVE_TH"),
        ("https://finance.yahoo.com/gainers?region=TH", "AUTO_LONG_TH")
    ]
    
    for url, m_type in targets:
        try:
            response = requests.get(url, headers=HEADERS)
            dfs = pd.read_html(response.text)
            df = dfs[0]
            
            # ‡∏î‡∏∂‡∏á 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î
            for symbol in df['Symbol'].head(10):
                clean_sym = symbol.split('.')[0] # ‡∏ï‡∏±‡∏î‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î‡πÑ‡∏ó‡∏¢ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏¥‡∏° .BK ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ yfinance ‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å
                if "_TH" in m_type and ".BK" not in symbol:
                    final_ticker = f"{clean_sym}.BK"
                elif "_TH" in m_type and ".BK" in symbol:
                    final_ticker = symbol # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢
                else:
                    final_ticker = clean_sym # ‡∏ï‡∏•‡∏≤‡∏î US ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•

                tickers.append({"ticker": final_ticker, "market_type": m_type})
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error scraping {url}: {e}")
            pass
        
    return tickers

# ---------------------------------------------------------
# 3. ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏á (Manual Control via GitHub)
# ---------------------------------------------------------

def get_user_manual_list(filename, type_name):
    """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt ‡∏à‡∏≤‡∏Å GitHub ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"""
    print(f"üåï Fetching '{filename}' from User GitHub...")
    tickers = []
    try:
        url = f"{REPO_BASE_URL}/{filename}"
        if "YOUR_GITHUB_USER" in url: return [] # ‡∏Å‡∏±‡∏ô User ‡∏•‡∏∑‡∏°‡πÅ‡∏Å‡πâ URL
        
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            lines = response.text.splitlines()
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Comment
            clean_lines = [line.strip() for line in lines if line.strip() and not line.startswith("#")]
            tickers = [{"ticker": t, "market_type": type_name} for t in clean_lines]
            print(f"   ‚úÖ Found {len(tickers)} items in {filename}")
    except: pass
    return tickers

# ---------------------------------------------------------
# MAIN EXECUTION
# ---------------------------------------------------------
def main():
    print("ü§ñ Starting Zero-Hardcode Scraper...")
    
    # 1. External Base (CSV/Wiki)
    base_data = get_external_sp500() + get_external_nasdaq100()
    
    # 2. Auto Hunters (Yahoo Live)
    hunter_data = get_market_movers()
    
    # 3. User Manual (GitHub Files)
    # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÉ‡∏ô GitHub ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    manual_data = get_user_manual_list("moonshots.txt", "MOONSHOT") + \
                  get_user_manual_list("favourites.txt", "FAVOURITE")
    
    all_data = base_data + hunter_data + manual_data
    
    if not all_data:
        print("‚ö†Ô∏è No data found! Check network or URLs.")
        return

    print(f"\nüíæ Syncing {len(all_data)} tickers to Supabase...")
    
    count = 0
    for item in all_data:
        try:
            # Upsert ‡∏•‡∏á DB
            supabase.table("ipo_trades").upsert({
                "ticker": item['ticker'],
                "market_type": item['market_type'],
                "status": "watching"
            }, on_conflict="ticker").execute()
            count += 1
            if count % 100 == 0: print(f"   ...synced {count}")
        except: pass

    print(f"‚úÖ SUCCESS: Synced {count} tickers.")
    print(f"   - External Base: {len(base_data)}")
    print(f"   - Auto Hunters: {len(hunter_data)}")
    print(f"   - User Manual: {len(manual_data)}")

if __name__ == "__main__":
    main()
